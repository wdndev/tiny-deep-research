# -*- coding: utf-8 -*-
import os
import asyncio
import typer
from functools import wraps
from prompt_toolkit import PromptSession
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich import print as rprint
from dotenv import load_dotenv, dotenv_values

from tiny_deep_research.deep_research import deep_research, write_final_report
from tiny_deep_research.feedback import generate_feedback
from tiny_deep_research.llm.llm_services import LLMService

load_dotenv(
    dotenv_path=".env", 
    override=True
)

app = typer.Typer()
console = Console()
session = PromptSession()

def coro(f):
    """Decorator to run async functions in a sync context.
    """
    @wraps(f)
    def wrapper(*args, **kwargs):
        return asyncio.run(f(*args, **kwargs))
    return wrapper

async def async_prompt(message: str, default: str = "") -> str:
    """ Asynchronous prompt function.
    """
    return await session.prompt_async(message, default=default)

@app.command()
@coro
async def main(
    concurrency: int = typer.Option(
        default=2, help="å¹¶å‘ä»»åŠ¡æ•°."
    ),
):
    """Deep Research CLI"""
    # æ ‡é¢˜
    console.print(
        Panel.fit(
            "[bold blue]Deep Research Assistant[/bold blue]\n"
            "[dim]An AI-powered research tool[/dim]"
        )
    )
    model_type = os.getenv("LLM_MODEL_TYPE", "")
    api_key = os.getenv("LLM_API_KEY", "")
    base_url = os.getenv("LLM_API_URL", "")
    model_name = os.getenv("LLM_MODEL_NAME", "")

    print("[SYS]: LLM_MODEL_TYPE: ", model_type)
    print("[SYS]:    LLM_API_URL: ", base_url)
    print("[SYS]: LLM_MODEL_NAME: ", model_name)

    console.print(f"ğŸ› ï¸ ä½¿ç”¨ [bold green]{model_type.upper()}[/bold green] æ¨¡å‹æœåŠ¡.")

    # æ¨¡å‹åˆå§‹åŒ–
    llm_client = LLMService(
        model_type=model_type,
        api_key=api_key,
        base_url=base_url,
        model_name=model_name
    )

    # äº¤äº’å¼è·å–ç”¨æˆ·è¾“å…¥
    query = await async_prompt("\nğŸ” æ‚¨çš„ç ”ç©¶é—®é¢˜æ˜¯: ")
    console.print()

    # è·å–æœç´¢æ·±åº¦å’Œå¹¿åº¦
    breadth_prompt = "ğŸ“Š ç ”ç©¶å¹¿åº¦ (æ¨è 2-10) [4]: "
    breadth = int((await async_prompt(breadth_prompt)) or "4")
    console.print()

    depth_prompt = "ğŸ” ç ”ç©¶æ·±åº¦ (æ¨è 1-5) [2]: "
    depth = int((await async_prompt(depth_prompt)) or "2")
    console.print()

    # ç”Ÿæˆç ”ç©¶è®¡åˆ’
    console.print("\n[yellow]åˆ›å»ºç ”ç©¶è®¡åˆ’...[/yellow]")
    follow_up_questions = await generate_feedback(
        query=query,
        llm_client=llm_client, 
    )

    # æ”¶é›†åç»­é—®é¢˜ç­”æ¡ˆ
    console.print("\n[bold yellow]å­é—®é¢˜: [/bold yellow]")
    answers = []
    for i, question in enumerate(follow_up_questions, 1):
        console.print(f"\n[bold blue]Q{i}:[/bold blue] {question}")
        answer = await async_prompt("â¤ ç­”æ¡ˆ: ")
        answers.append(answer)
        console.print()

    # ç»„åˆæŸ¥è¯¢å‚æ•°
    combined_query = f"""
    Initial Query: {query}
    Follow-up Questions and Answers:
    {chr(10).join(f"Q: {q} A: {a}" for q, a in zip(follow_up_questions, answers))}
    """

    # æ‰§è¡Œç ”ç©¶é˜¶æ®µï¼ˆå¸¦åŠ¨æ€è¿›åº¦æ¡ï¼‰
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        # Do research
        task = progress.add_task(
            "[yellow]æ­£åœ¨ç ”ç©¶æ‚¨çš„é—®é¢˜...[/yellow]", total=None
        )
        research_results = await deep_research(
            query=combined_query,
            breadth=breadth,
            depth=depth,
            concurrency=concurrency,
            llm_client=llm_client,
        )
        progress.remove_task(task)

        # å±•ç¤ºæˆæœ
        console.print("\n[yellow]é—®é¢˜è¦ç‚¹:[/yellow]")
        for learning in research_results["learnings"]:
            rprint(f"â€¢ {learning}")

        # ç”ŸæˆæŠ¥å‘Š
        task = progress.add_task("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...", total=None)
        report = await write_final_report(
            prompt=combined_query,
            learnings=research_results["learnings"],
            visited_urls=research_results["visited_urls"],
            llm_client=llm_client,
        )
        progress.remove_task(task)

        # Show results
        console.print("\n[bold green]ç ”ç©¶å®Œæˆ![/bold green]")
        console.print("\n[yellow]æœ€ç»ˆæŠ¥å‘Š:[/yellow]")
        console.print(Panel(report, title="Research Report"))

        # Show sources
        console.print("\n[yellow]æ¥æº:[/yellow]")
        for url in research_results["visited_urls"]:
            rprint(f"â€¢ {url}")

        # Save report
        output_path = f"outputs/report_{query[:30].replace(' ', '_')}.md"
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w") as f:
            f.write(report)
        console.print("\n[dim]æŠ¥å‘Šå·²ä¿å­˜åˆ° output.md æ–‡ä»¶[/dim]")

def run():
    """Synchronous entry point for the CLI tool."""
    asyncio.run(app())


# if __name__ == "__main__":
#     # asyncio.run(app())
#     import platform
#     if platform.system().lower() == 'windows':
#         loop = asyncio.get_event_loop()
#         loop.run_until_complete(app())
#     else:
#         asyncio.run(app())

if __name__ == "__main__":
    import platform
    # Windowsç³»ç»Ÿéœ€è¦ç‰¹æ®Šå¤„ç†
    if platform.system().lower() == 'windows':
        # è®¾ç½®æ­£ç¡®çš„äº‹ä»¶å¾ªç¯ç­–ç•¥
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        loop = asyncio.ProactorEventLoop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(app())
        finally:
            loop.close()
            asyncio.set_event_loop_policy(None)  # é‡ç½®ç­–ç•¥
    else:
        asyncio.run(app())

